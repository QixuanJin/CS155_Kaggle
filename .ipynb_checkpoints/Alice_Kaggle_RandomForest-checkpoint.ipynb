{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv into numpy array\n",
    "def load_data(filename, skiprows = 1):\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"train_2008.csv\")\n",
    "test_data = load_data(\"test_2008.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the training data, we ignore the first few columns (id, survey year, etc)\n",
    "X = train_data[:, 3:382]\n",
    "Y = train_data[:, 382] \n",
    "test = test_data[:, 3:382]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize both the training and testing distribution according to the training data distribution\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter tuning: number of estimators\n",
    "estimators = np.arange(35, 56)\n",
    "\n",
    "# Number of folds to perform cross validation over\n",
    "fold = 5\n",
    "# N_jobs = -1 to speed up computation, doesn't limit number of processors on CPU\n",
    "clf = RandomForestClassifier(criterion = 'gini', n_jobs = -1)\n",
    "\n",
    "test_score = []\n",
    "train_score = []\n",
    "for n in estimators:\n",
    "    # Useful output (also assures you that the program is actually running)\n",
    "    print(\"\\nEstimators: \" + repr(n))\n",
    "    # For taking the average later\n",
    "    tot_train = 0\n",
    "    tot_test = 0\n",
    "    # Set our parameter\n",
    "    clf.set_params(n_estimators=n)\n",
    "    \n",
    "    kf = KFold(n_splits=fold)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Useful output (also assures you that the program is actually running)\n",
    "        print(\".\", end=\"\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        clf.fit(X_train, Y_train)\n",
    "        \n",
    "        # Use our model to make predictions\n",
    "        train_prob = clf.predict_proba(X_train)\n",
    "        test_prob = clf.predict_proba(X_test)\n",
    "        # The second column is the probabilities of class 1\n",
    "        # We calculate the AUC score (higher is better)\n",
    "        tot_train += roc_auc_score(Y_train, train_prob[:, 1])\n",
    "        tot_test += roc_auc_score(Y_test, test_prob[:, 1])\n",
    "    # Store the average AUC across k folds for each parameter    \n",
    "    train_score.append(tot_train / fold)\n",
    "    test_score.append(tot_test / fold)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(estimators, test_score, label='Testing score')\n",
    "plt.plot(estimators, train_score, label='Training score')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('AUC score')\n",
    "plt.title('Random Forest with Gini Impurity and Number of Estimators')\n",
    "plt.legend(loc=0, shadow=True, fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "# Find the parameter that causes the maximum AUC score\n",
    "print('Test score maximized at estimator number = %i' % estimators[np.argmax(test_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter tuning: minimum leaf node\n",
    "min_samples_leaf = np.arange(20, 50)\n",
    "\n",
    "# Number of folds to perform cross validation over\n",
    "fold = 5\n",
    "# N_jobs = -1 to speed up computation, doesn't limit number of processors on CPU\n",
    "clf = RandomForestClassifier(n_estimators = 35, criterion = 'gini', n_jobs = -1)\n",
    "\n",
    "test_score = []\n",
    "train_score = []\n",
    "for leaf in min_samples_leaf:\n",
    "    # Useful output (also assures you that the program is actually running)\n",
    "    print(\"\\nleaf number: \" + repr(leaf))\n",
    "    # For taking the average later\n",
    "    tot_train = 0\n",
    "    tot_test = 0\n",
    "    # Set our parameter\n",
    "    clf.set_params(min_samples_leaf=leaf)\n",
    "    \n",
    "    kf = KFold(n_splits=fold)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Useful output (also assures you that the program is actually running)\n",
    "        print(\".\", end=\"\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        clf.fit(X_train, Y_train)\n",
    "        \n",
    "        # Use our model to make predictions \n",
    "        train_prob = clf.predict_proba(X_train)\n",
    "        test_prob = clf.predict_proba(X_test)\n",
    "        # The second column is the probabilities of class 1\n",
    "        # We calculate the AUC score (higher is better)\n",
    "        tot_train += roc_auc_score(Y_train, train_prob[:, 1])\n",
    "        tot_test += roc_auc_score(Y_test, test_prob[:, 1])\n",
    "    # Store the average AUC across k folds for each parameter     \n",
    "    train_score.append(tot_train / fold)\n",
    "    test_score.append(tot_test / fold)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(min_samples_leaf, test_score, label='Testing score')\n",
    "plt.plot(min_samples_leaf, train_score, label='Training score')\n",
    "plt.xlabel('Minimum Node Size')\n",
    "plt.ylabel('AUC score')\n",
    "plt.title('Random Forest with Gini Impurity and Minimum Node Size')\n",
    "plt.legend(loc=0, shadow=True, fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "# Find the parameter that causes the maximum AUC score\n",
    "print('Test score maximized at min_samples_leaf = %i' % min_samples_leaf[np.argmax(test_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter tuning: maximum depth\n",
    "max_depth = np.arange(2, 21)\n",
    "\n",
    "# Number to perform cross validation over\n",
    "fold = 5\n",
    "clf = RandomForestClassifier(n_estimators = 35, min_samples_leaf = 26, criterion = 'gini', n_jobs = -1)\n",
    "\n",
    "test_score = []\n",
    "train_score = []\n",
    "for depth in max_depth:\n",
    "    print(\"\\ndepth number: \" + repr(depth))\n",
    "\n",
    "    tot_train = 0\n",
    "    tot_test = 0\n",
    "    index = 0\n",
    "    clf.set_params(max_depth=depth)\n",
    "    kf = KFold(n_splits=fold)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(\".\", end=\"\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        clf.fit(X_train, Y_train)\n",
    "        \n",
    "        # the second column is for the probability for positive \n",
    "        train_prob = clf.predict_proba(X_train)\n",
    "        test_prob = clf.predict_proba(X_test)\n",
    "        tot_train += roc_auc_score(Y_train, train_prob[:, 1])\n",
    "        tot_test += roc_auc_score(Y_test, test_prob[:, 1])\n",
    "        index += 1\n",
    "    train_score.append(tot_train / fold)\n",
    "    test_score.append(tot_test / fold)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(max_depth, test_score, label='Testing score')\n",
    "plt.plot(max_depth, train_score, label='Training score')\n",
    "plt.xlabel('Maximum Depth')\n",
    "plt.ylabel('AUC score')\n",
    "plt.title('Random Forest with Gini Impurity and Maximum Depth')\n",
    "plt.legend(loc=0, shadow=True, fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "print('Test error maximized at max_depth = %i' % max_depth[np.argmax(test_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tinkering in general with the model parameters\n",
    "# K fold validation to provide a test score\n",
    "fold = 5\n",
    "estimators = 58\n",
    "leaf = 26\n",
    "depth = 17\n",
    "\n",
    "tot_train = 0\n",
    "tot_test = 0\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = estimators, min_samples_leaf = leaf, \\\n",
    "                                 max_depth = depth, criterion = 'gini', n_jobs = -1)\n",
    "kf = KFold(n_splits=fold)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\".\", end=\"\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    train_prob = clf.predict_proba(X_train)\n",
    "    test_prob = clf.predict_proba(X_test)\n",
    "    tot_train += roc_auc_score(Y_train, train_prob[:, 1])\n",
    "    tot_test += roc_auc_score(Y_test, test_prob[:, 1])\n",
    "\n",
    "print(\"\\nTraining auc: \" + repr(tot_train / fold))\n",
    "print(\"Testing auc: \" + repr(tot_test / fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training auc: 0.8290784840335808\n"
     ]
    }
   ],
   "source": [
    "# Running final tuned model\n",
    "estimators = 58\n",
    "leaf = 26\n",
    "depth = 17\n",
    "\n",
    "# Define our model\n",
    "clf = RandomForestClassifier(n_estimators = estimators, min_samples_leaf = leaf, \\\n",
    "                                 max_depth = depth, criterion = 'gini', n_jobs = -1)\n",
    "# Fit model on training data \n",
    "clf.fit(X, Y)\n",
    "\n",
    "# Predictions for training\n",
    "# the second column is the probabilities of class 1\n",
    "train_prob = clf.predict_proba(X)\n",
    "print(\"Training auc: \" + repr(roc_auc_score(Y, train_prob[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prob = clf.predict_proba(test)\n",
    "# Get column for the predictions of probabilities of being 1\n",
    "prob_ones = test_prob[:, 1]\n",
    "# Add first column of id that correspond with the predictions\n",
    "prob_ones = np.transpose(np.vstack((test_data[:, 0].astype(np.int32), prob_ones)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the test_prob as a csv file in the proper format\n",
    "np.savetxt(\"predictions2.csv\", prob_ones, fmt = '%d,%21.20f', delimiter=',', header = 'id,target', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
