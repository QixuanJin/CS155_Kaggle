{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, skiprows = 1):\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"train_2008.csv\")\n",
    "test_data = load_data(\"test_2008.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the training data\n",
    "X = train_data[:, 3:382]\n",
    "Y = train_data[:, 382] \n",
    "test = test_data[:, 3:382]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize both the training and testing distribution according to the training data\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(379, input_shape=(379,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(189))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(94))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(47))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(24))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(12))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 5\n",
    "tot_train = 0\n",
    "tot_test = 0\n",
    "tot_train_auc = 0 \n",
    "tot_test_auc = 0\n",
    "\n",
    "kf = KFold(n_splits=fold)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\".\", end=\"\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    fit = model.fit(X_train, Y_train, batch_size=64, epochs=10, verbose=0)\n",
    "    score_train = model.evaluate(X_train, Y_train, verbose=0)\n",
    "    score_test = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    # We only keep track of the accuracy\n",
    "    tot_train += score_train[1]\n",
    "    tot_test += score_test[1]\n",
    "    tot_train_auc += roc_auc_score(Y_train, model.predict(X_train, batch_size=64))\n",
    "    tot_test_auc += roc_auc_score(Y_test, model.predict(X_test, batch_size=64))\n",
    "\n",
    "print('\\nTrain accuracy:', tot_train/fold)\n",
    "print('Test accuracy:', tot_test/fold)\n",
    "print('Train AUC:', tot_train_auc/fold)\n",
    "print('Test AUC:', tot_test_auc/fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model.fit(X, Y, batch_size=64, epochs=20, verbose=1)\n",
    "score = model.evaluate(X, Y, verbose=0)\n",
    "print(\"Training accuracy:\", score[1])\n",
    "train_results = model.predict(X, batch_size=64)\n",
    "print(\"Training AUC:\", roc_auc_score(Y, train_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.predict(test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ones = np.hstack((np.reshape(test_data[:, 0], (test_data.shape[0], 1)), test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the test_prob as a csv file in the proper format\n",
    "np.savetxt(\"predictions6.csv\", prob_ones, fmt = '%d,%21.20f', delimiter=',', header = 'id,target', comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
