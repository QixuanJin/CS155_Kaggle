{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, skiprows = 1):\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"train_2008.csv\")\n",
    "test_data = load_data(\"test_2012.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts column c of data to array where each row is a 0-1 vector\n",
    "def ohe_col(train, test, c):\n",
    "    cats = np.unique(np.vstack((np.reshape(train[:,c], (train.shape[0], 1)), np.reshape(test[:, c], (test.shape[0], 1)))))\n",
    "    print(cats)\n",
    "    print(len(cats))\n",
    "    d = {}\n",
    "    for i in range(len(cats)):\n",
    "        d[cats[i]] = i\n",
    "    train_col, test_col = np.zeros((train.shape[0], len(cats))), np.zeros((test.shape[0], len(cats)))\n",
    "    for i in range(train.shape[0]):\n",
    "        train_col[i, d[train[i, c]]] = 1\n",
    "    for i in range(test.shape[0]):\n",
    "        test_col[i, d[test[i, c]]] = 1\n",
    "    return train_col, test_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.   2.   4. 201. 203.]\n",
      "5\n",
      "[1. 2. 3.]\n",
      "3\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12.]\n",
      "12\n",
      "[-3. -2. -1.  1.  2.]\n",
      "5\n",
      "[-1.  0.  1.]\n",
      "3\n",
      "[-1.  1.  2.]\n",
      "3\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "10\n",
      "[1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "8\n",
      "[-1.  1.  2.]\n",
      "3\n",
      "[0. 2. 3.]\n",
      "3\n",
      "[83001. 83002. 83003. 83004. 83006. 83011. 83021. 83031. 83041. 83051.\n",
      " 83101. 83111. 83131. 83141. 83201. 83241. 83251. 83261. 83262. 85001.\n",
      " 85002. 85003. 85011. 85021. 85031. 85041. 85231. 85241. 85251. 85261.\n",
      " 89001. 89002. 89003. 89004. 89011. 89012. 89021. 89031. 89251. 89252.\n",
      " 89261. 89262. 91001. 91002. 91003. 91011. 91021. 91031. 91251. 91261.]\n",
      "50\n",
      "[-3. -2. -1.  1.  2.]\n",
      "5\n",
      "[1. 2. 3. 4.]\n",
      "4\n",
      "[11. 12. 13. 14. 15. 16. 21. 22. 23. 31. 32. 33. 34. 35. 41. 42. 43. 44.\n",
      " 45. 46. 47. 51. 52. 53. 54. 55. 56. 57. 58. 59. 61. 62. 63. 64. 71. 72.\n",
      " 73. 74. 81. 82. 83. 84. 85. 86. 87. 88. 91. 92. 93. 94. 95.]\n",
      "51\n",
      "[ 1.  2.  4.  5.  6.  8.  9. 10. 11. 12. 13. 15. 16. 17. 18. 19. 20. 21.\n",
      " 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39.\n",
      " 40. 41. 42. 44. 45. 46. 47. 48. 49. 50. 51. 53. 54. 55. 56.]\n",
      "51\n",
      "[    0. 10420. 10500. 10580. 10740. 10900. 11020. 11100. 11300. 11340.\n",
      " 11460. 11500. 11540. 11700. 12020. 12060. 12100. 12260. 12420. 12540.\n",
      " 12580. 12940. 13140. 13380. 13460. 13740. 13780. 13820. 14020. 14060.\n",
      " 14260. 14500. 14540. 14740. 15180. 15380. 15940. 15980. 16300. 16580.\n",
      " 16620. 16700. 16740. 16860. 16980. 17020. 17140. 17460. 17660. 17820.\n",
      " 17860. 17900. 17980. 18140. 18580. 19100. 19340. 19380. 19460. 19500.\n",
      " 19660. 19740. 19780. 19820. 20100. 20260. 20500. 20740. 20940. 21340.\n",
      " 21500. 21660. 21780. 22020. 22140. 22180. 22220. 22420. 22460. 22660.\n",
      " 22900. 23020. 23060. 23420. 23540. 24340. 24540. 24580. 24660. 24860.\n",
      " 25060. 25180. 25420. 25500. 25860. 26100. 26180. 26420. 26580. 26620.\n",
      " 26900. 26980. 27100. 27140. 27260. 27340. 27500. 27740. 27780. 27900.\n",
      " 28020. 28100. 28140. 28660. 28700. 28740. 28940. 29100. 29180. 29340.\n",
      " 29460. 29540. 29620. 29700. 29740. 29820. 29940. 30020. 30460. 30780.\n",
      " 30980. 31100. 31140. 31180. 31340. 31420. 31460. 31540. 32580. 32780.\n",
      " 32820. 32900. 33100. 33140. 33260. 33340. 33460. 33660. 33700. 33740.\n",
      " 33780. 33860. 34740. 34820. 34900. 34940. 34980. 35380. 35620. 35660.\n",
      " 36100. 36140. 36260. 36420. 36500. 36540. 36740. 36780. 37100. 37340.\n",
      " 37460. 37860. 37900. 37980. 38060. 38300. 38900. 38940. 39100. 39140.\n",
      " 39340. 39380. 39460. 39540. 39580. 39740. 39900. 40060. 40140. 40220.\n",
      " 40380. 40420. 40900. 40980. 41060. 41180. 41420. 41500. 41540. 41620.\n",
      " 41700. 41740. 41860. 41940. 42020. 42060. 42100. 42140. 42220. 42260.\n",
      " 42340. 42540. 42660. 43340. 43620. 43780. 43900. 44060. 44100. 44180.\n",
      " 44220. 44700. 45060. 45220. 45300. 45780. 45820. 45940. 46060. 46140.\n",
      " 46220. 46540. 46660. 46700. 46940. 47020. 47220. 47260. 47300. 47380.\n",
      " 47580. 47900. 47940. 48140. 48620. 49180. 49420. 49620. 49660. 70750.\n",
      " 70900. 71650. 71950. 72400. 72850. 73450. 74500. 75700. 76450. 76750.\n",
      " 77200. 77350. 78100. 78700. 79600.]\n",
      "265\n",
      "[  0.   1.   3.   5.   7.   9.  11.  13.  15.  17.  19.  21.  23.  25.\n",
      "  27.  29.  31.  33.  35.  37.  39.  41.  43.  45.  47.  49.  51.  53.\n",
      "  55.  57.  59.  61.  63.  65.  67.  69.  71.  73.  75.  77.  79.  81.\n",
      "  83.  85.  86.  87.  89.  91.  93.  95.  97.  99. 101. 103. 105. 107.\n",
      " 109. 111. 113. 115. 117. 119. 121. 123. 125. 127. 129. 133. 135. 137.\n",
      " 139. 141. 145. 147. 151. 153. 155. 161. 163. 165. 169. 173. 179. 183.\n",
      " 187. 189. 215. 251. 303. 309. 329. 375. 381. 439. 479. 510. 550. 650.\n",
      " 700. 710. 740. 760. 810.]\n",
      "103\n",
      "[1. 2. 3. 4.]\n",
      "4\n",
      "[1. 2. 3.]\n",
      "3\n",
      "[0. 1. 2. 3. 4. 5. 6. 7.]\n",
      "8\n",
      "[  0. 118. 176. 184. 212. 216. 220. 260. 266. 268. 272. 290. 294. 304.\n",
      " 348. 356. 376. 378. 408. 428. 450. 482. 488. 500. 548. 715. 720.]\n",
      "27\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 12. 13. 14. 15. 16. 17. 18.]\n",
      "17\n",
      "[1. 2. 3. 4. 5. 6.]\n",
      "6\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 23. 26.]\n",
      "23\n",
      "[-1.  1.  2.  3.  4.  5.]\n",
      "6\n",
      "[-1.  1.  2.  3.  5.  9.]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "train_ohe, test_ohe = train_data, test_data\n",
    "cs = [4, 10, 13, 26, 31, 36, 43, 53, 61, 65, 68, 118, 127, 131, 182, 233, 498, 601, 605, 608, 617, 644, 664, 675, 698, 704]\n",
    "for c in cs:\n",
    "    train_col, test_col = ohe_col(train_ohe, test_ohe, c)\n",
    "    train_ohe = np.hstack((np.hstack((train_ohe[:,:c], train_col)), train_ohe[:,c+1:]))\n",
    "    test_ohe = np.hstack((np.hstack((test_ohe[:,:c], test_col)), test_ohe[:,c+1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4.]\n",
      "[0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(train_data[:, 54]))\n",
    "print(np.unique(train_ohe[:, 712]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the training data\n",
    "X = train_ohe[:, 3:-1]\n",
    "Y = train_ohe[:, -1] \n",
    "test = test_ohe[:, 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64667, 1037)\n",
      "(82820, 1037)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(test.shape)\n",
    "f = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize both the training and testing distribution according to the training data\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1037)              1076406   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1037)              4148      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1037)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1037)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 518)               537684    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 518)               2072      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 518)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 518)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 259)               134421    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 259)               1036      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 259)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 259)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 129)               33540     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 129)               516       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 129)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 129)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,801,216\n",
      "Trainable params: 1,797,106\n",
      "Non-trainable params: 4,110\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(f, input_shape=(f,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(int(f / 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(int(f / 4)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(f / 8)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(f / 16)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(int(f / 32)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(int(f / 64)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....\n",
      "Train accuracy: 0.9245166815561401\n",
      "Test accuracy: 0.8432757945727227\n",
      "Train AUC: 0.9621030240037808\n",
      "Test AUC: 0.8737245872693601\n"
     ]
    }
   ],
   "source": [
    "fold = 5\n",
    "tot_train = 0\n",
    "tot_test = 0\n",
    "tot_train_auc = 0 \n",
    "tot_test_auc = 0\n",
    "\n",
    "kf = KFold(n_splits=fold)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\".\", end=\"\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    fit = model.fit(X_train, Y_train, batch_size=64, epochs=10, verbose=0)\n",
    "    score_train = model.evaluate(X_train, Y_train, verbose=0)\n",
    "    score_test = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    # We only keep track of the accuracy\n",
    "    tot_train += score_train[1]\n",
    "    tot_test += score_test[1]\n",
    "    tot_train_auc += roc_auc_score(Y_train, model.predict(X_train, batch_size=64))\n",
    "    tot_test_auc += roc_auc_score(Y_test, model.predict(X_test, batch_size=64))\n",
    "\n",
    "print('\\nTrain accuracy:', tot_train/fold)\n",
    "print('Test accuracy:', tot_test/fold)\n",
    "print('Train AUC:', tot_train_auc/fold)\n",
    "print('Test AUC:', tot_test_auc/fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "64667/64667 [==============================] - 55s 853us/step - loss: 0.1824 - acc: 0.9262\n",
      "Epoch 2/20\n",
      "64667/64667 [==============================] - 52s 809us/step - loss: 0.1744 - acc: 0.9300\n",
      "Epoch 3/20\n",
      "64667/64667 [==============================] - 60s 927us/step - loss: 0.1663 - acc: 0.9324\n",
      "Epoch 4/20\n",
      "64667/64667 [==============================] - 54s 833us/step - loss: 0.1629 - acc: 0.9335\n",
      "Epoch 5/20\n",
      "64667/64667 [==============================] - 53s 823us/step - loss: 0.1568 - acc: 0.9372\n",
      "Epoch 6/20\n",
      "64667/64667 [==============================] - 53s 823us/step - loss: 0.1519 - acc: 0.9391\n",
      "Epoch 7/20\n",
      "64667/64667 [==============================] - 52s 801us/step - loss: 0.1507 - acc: 0.9397\n",
      "Epoch 8/20\n",
      "64667/64667 [==============================] - 53s 815us/step - loss: 0.1461 - acc: 0.9418\n",
      "Epoch 9/20\n",
      "64667/64667 [==============================] - 53s 812us/step - loss: 0.1466 - acc: 0.9413\n",
      "Epoch 10/20\n",
      "64667/64667 [==============================] - 52s 807us/step - loss: 0.1386 - acc: 0.9449\n",
      "Epoch 11/20\n",
      "64667/64667 [==============================] - 53s 813us/step - loss: 0.1379 - acc: 0.9444\n",
      "Epoch 12/20\n",
      "64667/64667 [==============================] - 53s 812us/step - loss: 0.1384 - acc: 0.9447\n",
      "Epoch 13/20\n",
      "64667/64667 [==============================] - 50s 777us/step - loss: 0.1353 - acc: 0.9466\n",
      "Epoch 14/20\n",
      "64667/64667 [==============================] - 52s 808us/step - loss: 0.1307 - acc: 0.9488\n",
      "Epoch 15/20\n",
      "64667/64667 [==============================] - 65s 1ms/step - loss: 0.1291 - acc: 0.9490: 9s - loss: 0.1273 - acc: 0 - ETA: 9s -\n",
      "Epoch 16/20\n",
      "64667/64667 [==============================] - 59s 918us/step - loss: 0.1273 - acc: 0.9488\n",
      "Epoch 17/20\n",
      "64667/64667 [==============================] - 58s 891us/step - loss: 0.1251 - acc: 0.9502\n",
      "Epoch 18/20\n",
      "64667/64667 [==============================] - 59s 909us/step - loss: 0.1251 - acc: 0.9507\n",
      "Epoch 19/20\n",
      "64667/64667 [==============================] - 59s 908us/step - loss: 0.1207 - acc: 0.9521\n",
      "Epoch 20/20\n",
      "64667/64667 [==============================] - 59s 910us/step - loss: 0.1180 - acc: 0.9538\n",
      "Training accuracy: 0.9909072633646219\n",
      "Training AUC: 0.9996648756885868\n"
     ]
    }
   ],
   "source": [
    "fit = model.fit(X, Y, batch_size=64, epochs=20, verbose=1)\n",
    "score = model.evaluate(X, Y, verbose=0)\n",
    "print(\"Training accuracy:\", score[1])\n",
    "train_results = model.predict(X, batch_size=64)\n",
    "print(\"Training AUC:\", roc_auc_score(Y, train_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.predict(test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ones = np.hstack((np.reshape(test_data[:, 0], (test_data.shape[0], 1)), test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the test_prob as a csv file in the proper format\n",
    "np.savetxt(\"predictionsOHE2012_16.csv\", prob_ones, fmt = '%d,%21.20f', delimiter=',', header = 'id,target', comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
